{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LING1340 Todo 9\n",
    "Name: **Daniel Zheng**\n",
    "\n",
    "Email: **daniel.zheng@pitt.edu**\n",
    "\n",
    "#### Dataset\n",
    "Large Movie Review Dataset from [here](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "\n",
    "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. *The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).*\n",
    "\n",
    "\n",
    "This is a modified version of my previous homework, trying some other classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# useful libraries\n",
    "import numpy as np\n",
    "import re, glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading training data\n",
    "def load(filepath):\n",
    "    files = glob.glob(filepath)\n",
    "    raw = []\n",
    "    for file in files:\n",
    "        with open(file, encoding='utf-8') as f:\n",
    "            raw.append(f.read())\n",
    "    return raw\n",
    "train_neg_raw = load('/home/dan/Documents/ling1340/data/aclImdb/train/neg/*')\n",
    "train_pos_raw = load('/home/dan/Documents/ling1340/data/aclImdb/train/pos/*')\n",
    "test_neg_raw = load('/home/dan/Documents/ling1340/data/aclImdb/test/neg/*')\n",
    "test_pos_raw = load('/home/dan/Documents/ling1340/data/aclImdb/test/pos/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative training sample: [ 'First let me say that I am not a Dukes fan, but after this movie the series looked like Law and Order. The worst thing was the casting of Roscoe and Boss Hogg. Burt Reynolds is not Boss Hogg, and even worse was M.C. Gainey as Roscoe, If they ever watched the show Roscoe was not a hard ass cop. He was more a Barney Fife than the role he played in this movie.<br /><br />The movie is loaded with the usual errors, cars getting torn up, and continues like nothing happened. The worst example of this is when the the General gets together with Billy Prickett, and the General is ran into a dirt hill obviously slowing to a near stop, but goes on to win the race.']\n"
     ]
    }
   ],
   "source": [
    "print('Negative training sample:', np.random.choice(train_neg_raw, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive training sample:  [ '**SPOILERS** Highly charge police drama about a serial killer loose in and around the small town of Riverside Wisconsin. Who\\'s being tracked down by the local police using policewoman Gina Pulasky, Helen Hunt,as an undercover decoy to catch him. <br /><br />Nothing new in this made for TV movie that you haven\\'t seen before but the depth of the acting and screenplay is unusually good and brings out a lot about not only the killer but the policewoman\\'s, as well as her fellow policeman lover, state of mind.<br /><br />Having been put under psychiatric care after shooting an armed and unstable assailant, who attacked her partner with a rifle. Officer Palusky is given the task to go undercover to get close to murder suspect Kayle Timler, Steven Webber. After he was positively identified by the little girl Sahsa, Kim Kluznick,who saw him not far from where little Timmy Curtis was found stabbed, 18 times, to death the next day.<br /><br />Getting a job at the Mr. \"C\" Diner where Tim works Gina gets to become very friendly with him and later tells him, in order to get Tim to open up, about him possibly being the serial murderer that she once killed in a hit-and-run accident a 79 year old woman. <br /><br />Tim who is said to have a genius IQ doesn\\'t seem to pick up on Gina\\'s attempt to trap him even when he later sees her at a bowling alley with her fellow cops spending a night out. Playing some weird cat and mouse game with her Tim at one point get\\'s Gina, at knife point, to admit that she\\'s wired. But Gina tells him that she was forced to do it by the police to get a break and an early release from prison. Besides Tim\\'s instability and criminal actions we find out that Gina isn\\'t all there as well.She seems to be suffering from her being rejected by her father who left her, with a drunk and abusing mother, as a young girl that\\'s effecting her work as an undercover policewoman. <br /><br />There\\'s also the fact that Gina\\'s lover policeman Will McCaid (Jeff Fahey), who\\'s estranged from his wife and two kids, who\\'s also on the serial murder case is too overprotective of her. That causes Gina to almost blow her cover and that has her later being taken off her assignment. <br /><br />Put back on undercover duty by her boss Capt. Cheney (Dan Conway), over the objections of Officer McCaid, after another young boy, 12 year-old Davy Marish,was found murdered Gina finally get\\'s herself together and gets Tim to admit that he\\'s the person who\\'s responsible for the string of murders in the area. Gina does it by having a hidden tape recorder that she replaced the one that she gave to him to show how honest she is, hidden on her.<br /><br />The movie \"In the Company of Darkness\" wasn\\'t really that exceptional but the acting by Helen Hunt Jeff Fahey and especially Steven Webber was. It was these high caliber performances that lifted the film well above the average made for TV movie were used to seeing.']\n"
     ]
    }
   ],
   "source": [
    "print('Positive training sample: ', np.random.choice(train_pos_raw, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of Data\n",
    "There are train and test sets of data. Within `train` and `test`, there is a `neg` and `pos` folder each with 12,500 negative and positive samples. In the `train` folder there is also a folder called `unsup` with 50,000 examples for unsupervised learning.\n",
    "\n",
    "### Processing\n",
    "A lot of the data contains `<br>` tags from HTML, which will have to be cleaned up. I will weight using term frequency - inverse document frequency (tf-idf) and train a variety of classifiers\n",
    "\n",
    "### Expected problems with approach\n",
    "A lot of the movie review data is going to be background information on the movies that probably won't be helpful for learning sentiment. Even though tf-idf will deemphasize many common words like \"is\", it might emphasize rare background info even more than common reviewing terms like \"terrible\", \"fantastic\", etc... Since background info terms should have low-frequency, it shouldn't make a huge difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function clean up data, taking out punctuation, numbers and special characters\n",
    "def clean(raw_input):\n",
    "    # tokenize and remove invalid characters\n",
    "    cleaned = [' '.join([x for x in string.split() if re.sub('[a-zA-Z0-9_.,!\"\\'-/]', '', x) == '']) for string in raw_input]\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a while because tokenizing + pos_tag is slow\n",
    "train_neg = clean(train_neg_raw)\n",
    "train_pos = clean(train_pos_raw)\n",
    "test_neg = clean(test_neg_raw)\n",
    "test_pos = clean(test_pos_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Memoirs of a Geisha\" is a visually stunning melodrama that seems more like a camp, drag queen satire than anything to do with real first half of the film defensively keeps insisting that geishas are neither prostitutes nor concubines, that they are the embodiment of traditional Japanese beauty. But other than one breathtaking dance, the rest of the movie degenerates into \"Pretty Baby\" in Storyville territory, or at least Vashti and Esther in the Purim story, as all the women\\'s efforts at art and artifice are about entertaining much, much older, drunken boorish men. Maybe it is Japanese culture that is being prostituted, and not just to the American louts after World War it\\'s the strain of speaking in English, but Ziyi Zhang shows barely little of the great flare she demonstrated in \"House of Flying Daggers (Shi mian mai fu)\" and \"Hero (Ying xiong).\" Michelle Yeoh occasionally gets to project a glimmer of her assured performance in \"Crouching Tiger, Hidden Dragon (Wo hu cang long).\" Only Li Gong shows any real life. Otherwise, I kept picturing Charles Ludlam in various roles, or even Cillian Murphy, as in kabuki theater, particularly as the plot dragged down in cat fight after cat supposed love story has zero chemistry, mostly due to the age differences, and I mostly felt sorry for Ken Watanabe and hoped his Hollywood pay check compensated for his loss of dignity as the mysterious \"Chairman.\" I remember more emotion in \"Portrait of Jennie\" as the young girl is anxious to grow up into Jennifer Jones to please Joseph see brief glimpses of reality when the geishas pose with regular women as photographic attractions, and as an ageless Ziyi Zhang lives out the war years in a very colorful kimono dying operation. The finale has little sense of score includes many chopped up traditional melodies, with cello by Yo Yo Ma and violin by Yitzhack Pearlman instead of traditional instrumentation, that are beautiful to listen to in accompaniment to the lovely cinematography, as long as one completely ignores the plot and stiff my mind wandered, I wondered how the great Japanese directors of samurai movies would have dealt with this story, which probably would have been more formal, but a lot more emotional.', \"I saw this in the market place at the Cannes Film Festival. a real cheapo prod - nothing wrong with that but you have to make up for it with a bit of sex or gore or both. Larry Cohen. Young is an interesting actor - well done to the producers for hooking her I opening scene in the space-ship coming down is hilarious - you could picture all the crew hands shaking it around! ha - but I wish the people who made this well - at least it's not pretentious.\"]\n",
      "total negative training samples: 12500\n"
     ]
    }
   ],
   "source": [
    "# example of what the filtered data looks like\n",
    "print(train_neg[:2])\n",
    "print('total negative training samples:', len(train_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Elvira, Mistress Of The Dark\" is a sort of \"Harper Valley P.T.A.\" with touches of the supernatural. Elvira (Cassandra Peterson) walks off her job as television horror movie hostess after the new station\\'s owner gets fresh with her. She\\'s now relying on a Las Vegas show to carry her through, but learns she needs to come up with more money to get the show started. Things look hopeless to raise that money until she receives notice of her aunt\\'s death, which then takes Elvira to Massachusetts for the reading of the will. A house in need of repairs, a dog, and a cookbook are all that is left to her by her aunt, and again it seems Elvira is having trouble coming up with the money for the Las Vegas show. The adults of the small and narrow minded town make things worse by making things more difficult for Elvira. Only the local hunk (Daniel Greene), and a group of teenagers will befriend her. Elvira\\'s Uncle \"Vinnie\" (W. Morgan Sheppard), presses to make a deal with Elvira for the cookbook, but Elvira soon learns of her powerful heritage that includes spellcasting, and a couple very effective casseroles. Elvira no longer wants to sell the cookbook to her uncle, but he is determined to get his hands on it knowing of its power. Elvira then faces being burned at the stake on the town\\'s old charge of witchcraft, and the showdown between her and her uncle. The plot is pretty simple, but the humor and well developed characters keep it moving at a nice pace. \"Elvira, Mistress Of The Dark\" is full of cute, gross, bawdy, and clever humor carried through by the many sight gags, puns, props, songs, and parodies. The film\\'s touches of the occult make this one of the best horror parodies ever made. It is a well made film with terrific acting by all including Edie McClurg, and Jeff Conaway (of \"Grease.\") There are also nice special effects. Many people (including myself) wondered if the Elvira character could carry a feature film, and the answer is delightfully, YES!', \"This final Voyager episode begins 23 years in the future. Voyager has made it back home. In the many years it took to return tho, the Vulcan Tuvoks' mind has been destroyed. He carried a disease they were too late getting home to Janeway comes across aliens who have time travel technology. She realizes, there's a Warp Conduit in the Delta Quadrant that could bring Voyager home immediately - if she could go back in time and notify Voyager. There's one problem. The Conduit is deep inside Borg visits Tuvok. He's like a child. He scribbles tho, obsessed, working on math problems or movie reviews or something, he's convinced are important somehow. In the institution, Tuvok cries, asking for 'Janeway' to please, please come back to decides to commandeer a federation shuttle and equip it with weapons technology 20 years ahead of the Borg, in the hopes of going back in time and using this new technology to guide Voyager to the Warp she goes back in time and links up with Voyager, Janeway meets her younger self. The two captains disagree, arguing about the plan. The real-captain visits Tuvok asking him if it's true he has a brain disorder. Tuvok admits it's true, but it can't be cured by the facilities on the ship so he's kept it to young Captain agrees to the older Captains' plan. To increase their chances of success the older Janeway plans to distract the Borg with her shuttle craft. The Borg actually capture Janeway and her shuttle. The Borg Queen personally assimilates Captain Janeway. But Janeway's expected this! the Borg Queen has assimilated a virus into herself that kills her. With the Borg Queen dead Voyager makes it thru the Warp Conduit back to federation space.\"]\n",
      "total positive training samples: 12500\n"
     ]
    }
   ],
   "source": [
    "print(train_pos[:2])\n",
    "print('total positive training samples:', len(train_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some visualizations\n",
    "from collections import Counter\n",
    "neg_word_counts = Counter()\n",
    "pos_word_counts = Counter()\n",
    "for neg, pos in zip(train_neg, train_pos):\n",
    "    neg_word_counts.update(word.strip('.,?!\"\\'').lower() for word in neg.split())\n",
    "    pos_word_counts.update(word.strip('.,?!\"\\'').lower() for word in pos.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 most common negative words: [('the', 157957), ('a', 78161), ('and', 72124), ('of', 68541), ('to', 68384), ('is', 49610), ('in', 42569), ('this', 38937), ('i', 37661), ('it', 37591), ('that', 34877), ('was', 26097), ('movie', 23129), ('for', 21374), ('but', 20784), ('with', 20470), ('as', 19801), ('film', 17447), ('on', 16623), ('not', 15692), ('have', 15082), ('you', 14669), ('are', 14506), ('be', 14292), ('his', 12048), ('one', 12025), ('at', 11963), ('he', 11876), ('they', 11687), ('all', 11291), ('like', 10670), ('so', 10619), ('just', 10414), ('by', 10321), ('an', 10129), ('or', 9649), ('from', 9526), ('who', 9160), ('about', 8898), ('if', 8691), ('out', 8491), ('some', 8122), (\"it's\", 8084), ('there', 8073), ('her', 7713), ('no', 7674), ('has', 7551), ('even', 7377), ('what', 7373), ('good', 7032), ('bad', 6952), ('would', 6828), ('only', 6636), ('more', 6560), ('when', 6520), ('up', 6330), ('really', 6135), ('had', 6096), ('were', 5964), ('my', 5716), ('time', 5662), ('very', 5660), ('can', 5507), ('she', 5468), ('see', 5280), ('me', 5277), ('their', 5254), ('which', 5250), ('than', 5139), ('been', 5065), ('get', 4948), ('do', 4946), (\"don't\", 4922), ('much', 4910), ('because', 4874), ('story', 4837), ('-', 4658), ('any', 4602), ('we', 4587), ('people', 4557), ('into', 4550), ('could', 4532), ('make', 4526), ('how', 4505), ('then', 4480), ('made', 4274), ('other', 4165), ('first', 4086), ('too', 3955), ('will', 3933), ('them', 3863), ('plot', 3838), ('acting', 3833), ('movies', 3821), ('most', 3812), ('way', 3700), ('after', 3631), ('him', 3596), ('think', 3531), ('its', 3517)]\n"
     ]
    }
   ],
   "source": [
    "print('100 most common negative words:',neg_word_counts.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 most common positive words: [('the', 167590), ('and', 87674), ('a', 82322), ('of', 76368), ('to', 66210), ('is', 56859), ('in', 48939), ('it', 37489), ('that', 33782), ('i', 33488), ('this', 33409), ('as', 25406), ('with', 22869), ('for', 21958), ('was', 21785), ('but', 19853), ('film', 19278), ('movie', 17901), ('his', 17083), ('on', 16466), ('are', 14716), ('he', 14428), ('you', 14324), ('not', 13813), ('one', 12682), ('have', 12514), ('be', 12201), ('by', 11810), ('all', 11177), ('an', 11159), ('at', 10997), ('from', 10587), ('who', 10570), ('her', 10209), ('has', 9134), ('they', 9033), ('so', 8590), ('like', 8506), ('very', 8185), ('about', 8178), (\"it's\", 8029), ('out', 7651), ('more', 7364), ('good', 7293), ('some', 7265), ('or', 7261), ('when', 7197), ('what', 7045), ('just', 7005), ('she', 6835), ('if', 6663), ('story', 6381), ('there', 6230), ('my', 6206), ('great', 6188), ('their', 6076), ('time', 5863), ('which', 5860), ('up', 5738), ('see', 5737), ('can', 5503), ('really', 5405), ('also', 5339), ('well', 5322), ('would', 5276), ('will', 5156), ('had', 5123), ('only', 5067), ('we', 4951), ('me', 4907), ('most', 4759), ('were', 4740), ('him', 4735), ('-', 4705), ('than', 4689), ('even', 4686), ('other', 4679), ('much', 4537), ('into', 4526), ('first', 4524), ('its', 4495), ('no', 4297), ('people', 4240), ('get', 4208), ('been', 4163), ('best', 4162), ('how', 4109), ('love', 4099), ('because', 4006), ('way', 3786), ('do', 3745), ('life', 3711), ('many', 3681), ('films', 3628), ('after', 3618), ('made', 3612), ('them', 3580), ('think', 3553), ('two', 3546), ('too', 3417)]\n"
     ]
    }
   ],
   "source": [
    "print('100 most common positive words:',pos_word_counts.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency counts\n",
    "Looking at the processed data above, words like \"is\" and \"was\" are very common, as expected. The negative set also has negative words like \"awful\", \"terrible\", and \"stupid\", while the positive set has words like \"perfect\", \"excellent\", and \"beautiful\". This is a good sign! After applying tf-idf, it should be pretty easy for a classifier to determine sentiment.\n",
    "\n",
    "### Creating labels\n",
    "Train and test labels are assigned using `0` as the negative class and `1` as the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train_neg + train_pos # concatenate for vectorizing\n",
    "train_labels = [0]*len(train_neg) + [1]*len(train_pos) # labels\n",
    "test = test_neg + test_pos\n",
    "test_labels = [0]*len(test_neg) + [1]*len(test_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# should be the same as CountVectorizer combined with TfidfTransformer\n",
    "tfidf = TfidfVectorizer()\n",
    "train_vectors = tfidf.fit_transform(train)\n",
    "# already fit to training set, so just transform\n",
    "test_vectors = tfidf.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 72996)\n",
      "(25000, 72996)\n"
     ]
    }
   ],
   "source": [
    "print(train_vectors.shape)\n",
    "print(test_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB().fit(train_vectors, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB:  82.8 % accuracy\n"
     ]
    }
   ],
   "source": [
    "predicted = classifier.predict(test_vectors)\n",
    "print('Multinomial NB: ', np.mean(predicted == test_labels)*100, '% accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "82.8% accuracy is pretty good! Definitely better than expected. Just for fun, I put together my own test set of movie review strings to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "custom_test = [\"This movie was the worst. I hate it.\", \"Terrible acting. Negative, bland, uninteresting.\", \n",
    "               \"This movie was great, I really enjoyed the acting!\", \n",
    "               \"Amazing storyline, hilarious characters, and a shocking ending.\", \n",
    "               \"The vague plot was ridiculously boring, and put me to sleep.\"]\n",
    "custom_labels = [0,0,1,1,0]\n",
    "custom_test_vectors = tfidf.transform(clean(custom_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0 % accuracy on custom test set\n",
      "[0 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "custom_predictions = classifier.predict(custom_test_vectors)\n",
    "print(np.mean(custom_predictions == custom_labels)*100, '% accuracy on custom test set')\n",
    "print(custom_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "This code does a few things:\n",
    "1. Reads in movie review data so that each review is one string in a list\n",
    "2. Preprocesses, removing everything but adjectives and verbs within each review.\n",
    "3. Creates train and test tf-idf vectors\n",
    "4. Fits a naive-bayes classifier to the train vector\n",
    "5. Test on testing data\n",
    "So it looks like using tf-idf with a Multinomial Naive-Bayes classifier can pretty reliably guess binary sentiment of a movie review. Next we'll try another classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB:  82.128 % test accuracy\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bernoulli_clf = BernoulliNB().fit(train_vectors, train_labels)\n",
    "print('Bernoulli NB: ', np.mean(bernoulli_clf.predict(test_vectors) == test_labels)*100, '% test accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:  83.568 % test accuracy\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# tuned params a bit by hand\n",
    "random_forest_clf = RandomForestClassifier(n_estimators=100, max_depth=80, criterion=\"gini\", random_state=0).fit(train_vectors, train_labels)\n",
    "print('Random Forest: ', np.mean(random_forest_clf.predict(test_vectors) == test_labels)*100, '% test accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting:  81.024 % test accuracy\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# didn't bother optimizing this by hand\n",
    "gb_clf = GradientBoostingClassifier().fit(train_vectors, train_labels)\n",
    "print('Gradient Boosting: ', np.mean(gb_clf.predict(test_vectors) == test_labels)*100, '% test accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num, vec_size = train_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 72996)             0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 500)               36498500  \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 128)               64128     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 36,570,949\n",
      "Trainable params: 36,570,949\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 12s - loss: 0.5351 - acc: 0.8004    \n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 13s - loss: 0.1770 - acc: 0.9351    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2ce1076cc0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "\n",
    "# use functional api for sparse input matrix\n",
    "inputs = Input(shape=(vec_size,), sparse=True)\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "x = Dense(500, activation='relu')(inputs)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "#x = Dropout(0.5)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# train\n",
    "model.fit(train_vectors, train_labels, epochs=2, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s     \n",
      "Neural Network:  87.2239995003 % test accuracy\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_vectors, test_labels, batch_size=1000, verbose=1)\n",
    "print('Neural Network: ', score[1]*100, '% test accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some optimizations, these classifiers would probably all be pretty similar, though I'd expected the NN to do better with larger datasets. In this case, the neural net did much better, but only because I tuned the params by hand and used mostly defaults for the other classifiers. I would have liked to use some kind of recurrent model w/ LSTMs, since that seems to be what the state-of-the-art models in text classification usually use these days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s\n",
      "Neural Network:  100.0 % test accuracy\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(custom_test_vectors, custom_labels, batch_size=1000, verbose=1)\n",
    "print('Neural Network: ', score[1]*100, '% test accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02084433]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(tfidf.transform(['bad']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9377352]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(tfidf.transform(['good']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00937826]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(tfidf.transform(['terrible bad']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.39154819]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(tfidf.transform(['terrible amazing']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99096328]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(tfidf.transform(['great amazing']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to see what the raw predictions are for some short phrases! Terrible and amazing are kind of opposites, so it makes sense that the model was less certain about sentiment. I defined 1 as positive and 0 as negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
