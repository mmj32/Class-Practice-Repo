{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning 2\n",
    "Na-Rae Han, 10/19/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General machine learning work flow:\n",
    "1. Choose a class of model\n",
    "2. Choose model hyperparameters\n",
    "3. Fit the model to the training data (\"training\")\n",
    "4. Use the model to predict labels for new data\n",
    "    - If labels are known (test data, aka 'gold' data), evaluate the performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three types of ML:\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.01-what-is-machine-learning.html\n",
    "\n",
    "1. Regression: predicting continuous values\n",
    "2. Classification: predicting discrete labels\n",
    "3. **Clustering: inferring labels on unlabeled data**  <-- This one below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns on/off pretty printing \n",
    "%pprint\n",
    "\n",
    "# Every returned Out[] is displayed, not just the last one. \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn               # sklearn is the ML package we will use\n",
    "import seaborn as sns        # seaborn graphical package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering: a type of unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using sklearn's pre-loaded data set \"20 Newsgroups\" \n",
    "- Code below is adapted from sklearn's official tutorial: \n",
    "  http://scikit-learn.org/stable/auto_examples/text/document_clustering.html \n",
    "\n",
    "Topic-based clustering is our goal:  \n",
    "- Given a set of documents that are written on 4 topics, can they be grouped into 4 clusters? \n",
    "\n",
    "We will try **K-means clustering** method. \n",
    "- A good introduction article: https://www.datascience.com/blog/k-means-clustering\n",
    "- sklearn's documentation: http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_iris.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TfidfVectorizer is essentially CountVectorizer + TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# We will use the same 4 categories\n",
    "cats = ['talk.religion.misc', 'soc.religion.christian', 'sci.space', 'comp.graphics']\n",
    "\n",
    "# Not using train-test split. Because this is un-supervised! \n",
    "dataset = fetch_20newsgroups(subset='all', categories=cats, shuffle=True, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.target\n",
    "dataset.target[5]\n",
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our case, WE KNOW TRUE VALUE OF K: 4 topics. \n",
    "# But in many real-life use cases, true number of clusters will not be known,\n",
    "#  and user must experiment with different K values. \n",
    "\n",
    "true_k = np.unique(dataset.target).shape[0]\n",
    "print(true_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ignore words found in over 50% of documents, ignore words found in just 1 document. \n",
    "# 1000 most frequent words, remove stop words. \n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, max_features=1000, stop_words='english')\n",
    "X = vectorizer.fit_transform(dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[5]\n",
    "print(X[5])\n",
    "# 1x1000? \"sparse matrix\"? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_.get('space')\n",
    "vectorizer.get_feature_names()[204]\n",
    "vectorizer.get_feature_names()[180]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation complete. Time to apply K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1, verbose=True)\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bunch of metrics that compare target labels and labels as assigned by KM. \n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(dataset.target, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(dataset.target, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(dataset.target, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(dataset.target, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top terms (\"features\") as ranked by centroids\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.labels_[:20]        # Cluster labels as assigned by KMeans\n",
    "dataset.target[:20]    # These are the real target labels\n",
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 2. Let's try 3 clusters this time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km2 = KMeans(n_clusters=3, init='k-means++', max_iter=100, n_init=1, verbose=True)\n",
    "km2.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = km2.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(3):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()\n",
    "# Are the clusters looking better? \n",
    "# CAVEAT: could be local optimum, re-run to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km2.labels_[:20]        # Cluster labels as assigned by KMeans\n",
    "dataset.target[:20]     # These are the real target labels\n",
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newsgroup label -> KM label. Will need to adjust. \n",
    "labelmap = {0:0, 1:2, 2:1, 3:1}\n",
    "\n",
    "target_conv = [labelmap[x] for x in dataset.target]\n",
    "target_conv[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(target_conv, km2.labels_)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cm.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "plt.xlabel('true group')\n",
    "plt.ylabel('predicted group')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Can we produce nifty clustering visuals\n",
    "such as the ones in tutorial/documentation: \n",
    "- https://www.datascience.com/blog/k-means-clustering\n",
    "- http://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_iris_004.png\n",
    "\n",
    "??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Too Many Dimensions\n",
    "This is where PCA (Principal Component Analysis) comes in. \n",
    "- Textbook chapter: https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on new, made up examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = ['sending a payload to the ISS', 'I met Santa Claus once']\n",
    "preds = km2.predict(tests)\n",
    "print(preds)\n",
    "#???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = ['sending a payload to the ISS', 'I met Santa Claus once']\n",
    "tests_tfidf = vectorizer.transform(tests)    # Yep, need this\n",
    "preds = km2.predict(tests_tfidf)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
