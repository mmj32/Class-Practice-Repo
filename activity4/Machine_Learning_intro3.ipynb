{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning 3\n",
    "Na-Rae Han, 10/19/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond Naive Bayes: other ML methods?\n",
    "- We will try **SVM** (**Support Vector Machine**). \n",
    "- Textbook section on SVM: https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html\n",
    "\n",
    "## Feature engineering, categorical data\n",
    "- Feature engineering\n",
    "- Handling categorical data\n",
    "- Testbook section: https://jakevdp.github.io/PythonDataScienceHandbook/05.04-feature-engineering.html\n",
    "- A better explanation here: https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f\n",
    "- And here: http://pbpython.com/categorical-encoding.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns on/off pretty printing \n",
    "%pprint\n",
    "\n",
    "# Every returned Out[] is displayed, not just the last one. \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn               # sklearn is the ML package we will use\n",
    "import seaborn as sns        # seaborn graphical package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# We will use the same 4 categories\n",
    "cats = ['talk.religion.misc', 'soc.religion.christian', 'sci.space', 'comp.graphics']\n",
    "\n",
    "train = fetch_20newsgroups(subset='train', categories=cats)\n",
    "test = fetch_20newsgroups(subset='test', categories=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.target\n",
    "train.target[5]\n",
    "train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train.data)\n",
    "len(test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TfidfVectorizer is essentially CountVectorizer + TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Ignore words found in over 50% of documents, ignore words found in just 1 document. \n",
    "# 1000 most frequent words, remove stop words. \n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, max_features=1000, stop_words='english')\n",
    "X_train = vectorizer.fit_transform(train.data)\n",
    "y_train = train.target\n",
    "X_test = vectorizer.transform(test.data)\n",
    "y_test = test.target\n",
    "\n",
    "# Cumbersome. Alternative: using pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to build: Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "model = SVC(kernel='linear', C=1E10)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = model.predict(X_test)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:20]\n",
    "labels[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, labels)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cm.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=train.target_names, yticklabels=train.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is this better than Naive Bayes?\n",
    "What other classification algorithms are there?\n",
    "- Logistic Regression\n",
    "- K-Nearst Neighbors\n",
    "- Decision Tree, Random Forest https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html\n",
    "- Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Categorical data\n",
    "Margaret's ASL class data as an example:\n",
    "https://github.com/Data-Science-for-Linguists/Class-Practice-Repo/blob/master/todo3/asl-background-cut_margaret.csv\n",
    "\n",
    "(Make sure to click on \"Raw\" button to get to raw CSV file format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/Data-Science-for-Linguists/Class-Practice-Repo/master/todo3/asl-background-cut_margaret.csv'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: \n",
    "- Which columns are truly categorical? \n",
    "- How to best convert them to numerical values? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to ETS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ets_df = pd.read_csv('ets_df_final.csv', index_col=0)\n",
    "ets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "- Which columns are truly categorical?\n",
    "\n",
    "`L1`\n",
    "- As classification target?\n",
    "- As classificatoin feature?\n",
    "\n",
    "`Score`\n",
    "- As classification target?\n",
    "- As classification feature? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo-categorical data must be converted into numerical values\n",
    "`Score` should be converted into corresponding numbers, both as target and feature. \n",
    "- Low: 1\n",
    "- Medium: 2\n",
    "- High: 3\n",
    "\n",
    "In predicting `Score`, you will be building a **REGRESSION model**, not a classification model. \n",
    "- Predicted values will come out as 0.8, 1.2, 2.8, 3.1 etc.\n",
    "- The continuous numeric values will then have to be binned into discrete categories such as 1, 2, 3 or 'Low', 'Medium', 'High'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truly categorical data\n",
    "- Can be used as classification target as is. You will be building a true **CLASSIFICATION model**. \n",
    "- When used as *features*, they must be DECOMPOSED into a set of binary, independent features.\n",
    "   1. Option 1: Encode value labels as integers: map 'Korean' 0, 'German' to 1, 'Japanese' to 3 (through `LabelEncoder`)\n",
    "      - This step alone isn't enough. (WHY?)\n",
    "      - You must then encode map integer labels to a set of binary-valued columns (through `OneHotEncoder`)\n",
    "   2. Option 2. Transform the categorical column into a set of **dummy variables** (through `pandas.get_dummes`) \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: LabelEncoder + OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini = ets_df[:10][['Prompt', 'L1', 'Score']]\n",
    "mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenc = LabelEncoder()\n",
    "lenc.fit(mini['L1'])\n",
    "lenc.transform(mini['L1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini['L1'] = lenc.transform(mini['L1'])\n",
    "mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohenc = OneHotEncoder()\n",
    "ohenc.fit_transform(mini[['L1']]).toarray()\n",
    "#ohenc.fit(mini['L1'].values.reshape(-1,1))\n",
    "#ohenc.transform(mini['L1'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Use `pandas.get_dummies`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini = ets_df[:10][['Prompt', 'L1', 'Score']]\n",
    "mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(mini, columns=['L1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
